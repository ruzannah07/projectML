{"cells":[{"metadata":{"_uuid":"11c05a044f27e0cfd9d38927523d86f9fcd1ef02"},"cell_type":"markdown","source":"In this kernel, I will try building a CNN from scratch for multi-class classification for the fruits dataset. In this dataset, we have a total of 55244 images which are divided into two folders - training set of 41322 images and testing set of 13877 images. The size of the given images is 100 * 100. We have 81 classes of fruits.\nLet's get started!"},{"metadata":{"trusted":true,"_uuid":"8ae6b0f622f3371e629d3bc916d881b11b7a5bf8"},"cell_type":"code","source":"# First, we are going to load the file names and their respective target labels into numpy array! \nfrom sklearn.datasets import load_files\nimport numpy as np\n\ntrain_dir = '../input/fruits-360_dataset/fruits-360/Training'\ntest_dir = '../input/fruits-360_dataset/fruits-360/Test'\n\ndef load_dataset(path):\n    data = load_files(path)\n    files = np.array(data['filenames'])\n    targets = np.array(data['target'])\n    target_labels = np.array(data['target_names'])\n    return files,targets,target_labels\n    \nx_train, y_train,target_labels = load_dataset(train_dir)\nx_test, y_test,_ = load_dataset(test_dir)\nprint('Loading complete!')\n\nprint('Training set size : ' , x_train.shape[0])\nprint('Testing set size : ', x_test.shape[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f09063e1d62bcf1fe28eb840ee93fd95fcfdb6ed"},"cell_type":"code","source":"# Let's confirm the number of classes :p\nno_of_classes = len(np.unique(y_train))\nno_of_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27cf6d93da0e851e97f341be0dd7060eba55ce0f"},"cell_type":"code","source":"print(y_train[0:10])\n# target labels are numbers corresponding to class label. We need to change them to a vector of 81 elements.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"299c14e9e0a8fc0e5fde71bf144a012dee02160b"},"cell_type":"code","source":"from keras.utils import np_utils\ny_train = np_utils.to_categorical(y_train,no_of_classes)\ny_test = np_utils.to_categorical(y_test,no_of_classes)\ny_train[0] # Note that only one element has value 1(corresponding to its label) and others are 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726a160122475e7d47e0c00bf617b7a5c0838ba6"},"cell_type":"code","source":"# Now, we have to divide the validation set into test and validation set\nx_test,x_valid = x_test[7000:],x_test[:7000]\ny_test,y_vaild = y_test[7000:],y_test[:7000]\nprint('Vaildation X : ',x_valid.shape)\nprint('Vaildation y :',y_vaild.shape)\nprint('Test X : ',x_test.shape)\nprint('Test y : ',y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa2cbb8c65386cef369abc49c7a4c1da3957d08e"},"cell_type":"code","source":"x_train[0]\n# training data is just file names of images. We need to convert them into pixel matrix.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1b3ab256679300f07d026017477092ebc5c71594"},"cell_type":"code","source":"# We just have the file names in the x set. Let's load the images and convert them into array.\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\n\ndef convert_image_to_array(files):\n    images_as_array=[]\n    for file in files:\n        # Convert to Numpy Array\n        images_as_array.append(img_to_array(load_img(file)))\n    return images_as_array\n\nx_train = np.array(convert_image_to_array(x_train))\nprint('Training set shape : ',x_train.shape)\n\nx_valid = np.array(convert_image_to_array(x_valid))\nprint('Validation set shape : ',x_valid.shape)\n\nx_test = np.array(convert_image_to_array(x_test))\nprint('Test set shape : ',x_test.shape)\n\nprint('1st training image shape ',x_train[0].shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb44a8c41693a8e14332cc734dd2ced9d5582986"},"cell_type":"code","source":"print('1st training image as array',x_train[0]) # don't worry if you see only 255s..\n# there are elements will other values too :p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9bbd173bdc8646ebafeeb31e492c80199683c72"},"cell_type":"code","source":"# time to re-scale so that all the pixel values lie within 0 to 1\nx_train = x_train.astype('float32')/255\nx_valid = x_valid.astype('float32')/255\nx_test = x_test.astype('float32')/255\nx_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cb2bff712654024215ef79585dba06dd0114396","scrolled":true},"cell_type":"code","source":"#Let's visualize the first 10 training images!\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize =(30,5))\nfor i in range(10):\n    ax = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n    ax.imshow(np.squeeze(x_train[i]))\n# Yummy fruits ;) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d14849fa59a55c57dfdd9622a4d9bcc90de8971b"},"cell_type":"code","source":"#Simple CNN from scratch - we are using 3 Conv layers followed by maxpooling layers.\n# At the end we add dropout, flatten and some fully connected layers(Dense).\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.layers import Activation, Dense, Flatten, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 2,input_shape=(100,100,3),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 32,kernel_size = 2,activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 64,kernel_size = 2,activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 128,kernel_size = 2,activation= 'relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(150))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(81,activation = 'softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dff4d3931ad4ff2748d7681a9767007877694b12"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\nprint('Compiled!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ac38a4408414793435002ff4af1ab25bbf082a8"},"cell_type":"code","source":"batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath = 'cnn_from_scratch_fruits.hdf5', verbose = 1, save_best_only = True)\n\nhistory = model.fit(x_train,y_train,\n        batch_size = 32,\n        epochs=30,\n        validation_data=(x_valid, y_vaild),\n        callbacks = [checkpointer],\n        verbose=2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03edb7196563f35ee053d3306f594ece4f50139b"},"cell_type":"code","source":"# load the weights that yielded the best validation accuracy\nmodel.load_weights('cnn_from_scratch_fruits.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b9df1b9ce5cf750cff33b8ab975acee72c6aa4d"},"cell_type":"code","source":"# evaluate and print test accuracy\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n#98% accuracy !!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45b1a4b675f0c1189e8cbdd0876badac6fa1d6e"},"cell_type":"code","source":"# Let's visualize test prediction.\n\ny_pred = model.predict(x_test)\n\n# plot a random sample of test images, their predicted labels, and ground truth\nfig = plt.figure(figsize=(16, 9))\nfor i, idx in enumerate(np.random.choice(x_test.shape[0], size=16, replace=False)):\n    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(x_test[idx]))\n    pred_idx = np.argmax(y_pred[idx])\n    true_idx = np.argmax(y_test[idx])\n    ax.set_title(\"{} ({})\".format(target_labels[pred_idx], target_labels[true_idx]),\n                 color=(\"green\" if pred_idx == true_idx else \"red\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4754c40e12c2070c3721e46fba5465df9719151"},"cell_type":"code","source":"#Finally lets visualize the loss and accuracy wrt epochs\n\nimport matplotlib.pyplot as plt \nplt.figure(1)  \n   \n # summarize history for accuracy  \n   \nplt.subplot(211)  \nplt.plot(history.history['acc'])  \nplt.plot(history.history['val_acc'])  \nplt.title('model accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \n   \n # summarize history for loss  \n   \nplt.subplot(212)  \nplt.plot(history.history['loss'])  \nplt.plot(history.history['val_loss'])  \nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}